{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAT245 Lab 9\n",
    "\n",
    "## Classifcation using Logistic Regression\n",
    "\n",
    "### Background\n",
    "\n",
    "In a binary classification problem we have samples of data $x \\in \\mathbb{R}^n$, and we want to predict the value of a target variable $y \\in \\{0, 1\\}$. For instance, a farmer might want to know if a $32 \\times 32$ image $X \\in \\mathbb{R}^{32\\times 32}$ contained a picture of a cucumber or not. We model absense or presense of a cucumber with outputs of $0$ or $1$ respectively.\n",
    "\n",
    "The logistic regression approach to classification uses a hypothesis function $h_\\theta$ of the form\n",
    "\n",
    "$$\n",
    "h_\\theta(x) \n",
    "  = \n",
    "g(\\theta^T x)\n",
    "  =\n",
    "\\frac{1}{1 + e^{-\\theta^T x}}.\n",
    "$$\n",
    "\n",
    "The parameter $\\theta$ is what we're going to want to optimize. Since $h_\\theta(x) \\in [0, 1]$, we can interpret its value as the probability of $x$ having a certain label:\n",
    "\n",
    "\\begin{align*}\n",
    "  \\mathbb{P}(y = 1 ~|~ x, \\theta) &= h_\\theta(x) \\\\\n",
    "  \\mathbb{P}(y = 0 ~|~ x, \\theta) &= 1 - h_\\theta(x).\n",
    "\\end{align*}\n",
    "\n",
    "So if $h_\\theta(x) \\geq 0.5$, we predict $y =1$, otherwise we predict $y = 0$. Written differently, this is\n",
    "\n",
    "$$\n",
    " \\mathbb{P}(y ~|~ x, \\theta) = h_\\theta(x)^y (1 - h_\\theta(x))^{1-y}.\n",
    "$$\n",
    "\n",
    "Now, suppose we have $m$ independently generated samples in our dataset. As usual, we arrange these $m$ samples into an $m\\times n$ matrix whose rows each represent individual samples. The likelihood of the parameter $\\theta$ is given by\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\theta)\n",
    "  &=\n",
    "\\mathbb{P}(y ~|~ X, \\theta) \\\\\n",
    "  &= \n",
    "\\prod_{i=1}^m \\mathbb{P}(y^{(i)} ~|~ X^{(i)}, \\theta) \\\\\n",
    "  &=\n",
    "\\prod_{i=1}^m h_\\theta(x^{(i)})^{y^{(i)}} (1 - h_\\theta(x^{(i)}))^{1-y^{(i)}}.\n",
    "\\end{align*}\n",
    "\n",
    "Our goal is then to choose $\\theta$ to maximize this likelihood. In practice, it is easier to maximize the log-likelihood function:\n",
    "\n",
    "\\begin{align*}\n",
    "  l(\\theta) \n",
    "    &= \n",
    "  \\log(L(\\theta)) \\\\ \n",
    "    &= \n",
    "  \\sum_{i=1}^m y^{(i)} \\log [ h_\\theta(x^{(i)})] + (1 - y^{(i)}) \\log[1 - h_\\theta(x^{(i)})].\n",
    "\\end{align*}\n",
    "\n",
    "We can maximize the log-likelihood by performing stochastic gradient ascent. In other words, we choose a training pair $(x, y) = (x^{(i)}, y^{(i)})$ at random, and compute the gradient of $l$ at this pair using the formula:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial }{\\partial\\theta_j} l (\\theta)\n",
    "  &=\n",
    "\\left(y \\frac{1}{g(\\theta^T x} - (1 - y) \\frac{1}{1 - g(\\theta^T x)}\\right) \\frac{\\partial}{\\partial\\theta_j}g(\\theta^T x) \\\\\n",
    "  &=\n",
    "\\left(y \\frac{1}{g(\\theta^T x} - (1 - y) \\frac{1}{1 - g(\\theta^T x)}\\right) \n",
    "  g(\\theta^T x)(1 - g(\\theta^Tx)) \\frac{\\partial}{\\partial\\theta_j}\\theta^Tx \\\\\n",
    "  &=\n",
    "(y(1 - g(\\theta^Tx)) - (1 - y)g(\\theta^Tx))x_j \\\\\n",
    "  &=\n",
    "(y - h_\\theta(x))x_j.\n",
    "\\end{align*}\n",
    "\n",
    "Above we used the derivative identity $g'(x) = g(z)(1-g(z))$. To choose new $\\theta$ values, we want to take a small step in the direction of the gradient (since we are *maximizing* $l(\\theta)$). This gives the update rule of\n",
    "\n",
    "$$\n",
    "\\theta_j = \\theta_j + \\alpha (y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate parameter. \n",
    "\n",
    "### Application: breast cancer detection\n",
    "\n",
    "The `sklearn` breast cancer dataset consists of $569$ $30$-dimensional data points. The goal is to classify each data point as representing either a *malignant* or *benign* tumor. You can load the data with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "samples, targets = bc.data, bc.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Goals (1):\n",
    "\n",
    "- Split the breast cancer data into 70% training and 30% validation sets. \n",
    "- Write a `python` implementation of the logistic regression function $(\\theta, x) \\mapsto h_\\theta(x)$. \n",
    "- Implement the stochastic gradient ascent (SGA) algorithm described above to choose the best parameter $\\theta$ for the hypothesis function $h_\\theta$. How do different learning rates affect convergence? Typical choices are in the range 0.001 - 0.1. \n",
    "- Validate your model's classification accuracy on the validation set (the `sklearn.metrics.accuracy_score` function may come in handy here).\n",
    "- How many iterations of SGA do you need to consistently get >85% classification accuracy on the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "### Background\n",
    "\n",
    "Principal component analysis (PCA) is a dimensionality reduction technique. The idea is to project the data down to lower dimension by 'dropping' those directions/dimensions that don't contain much variance. For instance, consider the following sample of data points in 2D:\n",
    "\n",
    "<img src=\"pca.svg\" alt=\"Gaussian data in 2D\" style=\"width: 300px;\"/>\n",
    "\n",
    "The goal of a PCA in this case would be to project all of the data points onto the axis spanned by the longer arrow; since the short arrow is orthogonal to the large one, it would be ideal if we could project along the short arrow. The new dataset will be 1-dimensional, and since most of the variation in the data was along the direction spanned by the long arrow, hopefully we haven't lost much information.\n",
    "\n",
    "For more details about the mathematics of PCA, see Andrew Ng's great notes [here](http://cs229.stanford.edu/notes/cs229-notes10.pdf).\n",
    "\n",
    "### Identifying digits with PCA and k-Nearest Neighbors.\n",
    "\n",
    "The `sklearn` digits dataset contains images of handwritten digits, much like the famous MNIST dataset. Here's a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "samples, targets = digits.data, digits.target\n",
    "\n",
    "%matplotlib inline\n",
    "plt.imshow(np.reshape(samples[0], (8,8)), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are each 8x8, for a total number of 64 dimensions. \n",
    "\n",
    "### Goals (2):\n",
    "- Split the digits dataset into 70% training and 30% validation.\n",
    "- Use `sklearn`'s PCA implementation to reduce the dimensionality of the digits dataset (see `sklearn.decomposition.PCA`).\n",
    "- Once you've used PCA to reduce the dimensionality of the entire dataset, use the k-nearest neighbor algorithm to classify the digits by finding the class of the digits nearest to your reduced example (see `sklearn.neighbors.KNeighborsClassifier`). \n",
    "- Validate your PCA + kNN model on the test set. How does accuracy change with the number of principal components you select?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
