{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAT245 Lab 8\n",
    "\n",
    "## Curse of dimensionality - volume of a hypercube\n",
    "\n",
    "Suppose we have a $p$-dimensional dataset containing $N$ observations that we've normalized so that all points are contained in the $p$-dimensional unit cube. Fix a target data point $x_0$, and say we send out a hypercubical neighborhood $C(x_0)$ about $x_0$ to capture a fraction $r$ of the observations. In other words, we want to consider a cubical neighbourhood of $x_0$ with volume $0 < r < 1$ (see image below for an example with $x_0 = 0$).\n",
    "\n",
    "<img src=\"cube.png\">\n",
    "\n",
    "The volume of a $p$-dimensional cube with side-length $l$ is given by $l^n$, so we will have to choose the side-length of our smaller cube $C(x_0)$ to be $r^\\frac{1}{p}$.\n",
    "\n",
    "If we add the additional assumption that the points in our dataset are uniformly distributed, we can compute the median distance from the origin to the closest data point by\n",
    "\n",
    "$$\n",
    "d(p, N) = \\left( 1 - \\frac{1}{2^{\\frac{1}{N}}}\\right)^{\\frac{1}{p}}.\n",
    "$$\n",
    "\n",
    "[Formula Explanation](https://stats.stackexchange.com/questions/130998/explanation-of-formula-for-median-closest-point-to-origin-of-n-samples-from-unit) for hypersphere, follows for hypercube.\n",
    "\n",
    "#### Goals (1):\n",
    "\n",
    "- For $p = 2, 10,$ and $100$, plot the side-length of $C(x_0)$ against the fraction of the $p$-unit cube covered by $C(x_0)$. \n",
    "- For fixed $N$, say $N = 500$, plot $p \\mapsto d(p, 500)$ as $p$ ranges between $0 < p < 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Gaussian vectors become orthogonal in high dimension.\n",
    "\n",
    "Suppose $Y$ and $Z$ are $p$-dimensional random vectors where each entry has been drawn from the standard normal distribution $\\mathcal{N}(0, 1)$. In other words\n",
    "\n",
    "$$\n",
    "Y_i \\sim \\mathcal{N}(0, 1) \\quad \\text{and} \\quad Z_i \\sim \\mathcal{N}(0, 1) \\quad \\forall i.\n",
    "$$\n",
    "\n",
    "It turns out that as $d \\to \\infty$, the dot product of the normalized vectors $\\widehat{Y} = Y / \\|Y\\|$ and $\\widehat{Z} = Z / \\|Z\\|$ tends to zero. In other words $\\widehat{Y}$ and $\\widehat{Z}$ become approximately orthogonal in high dimension. Our next goal is to verify this empirically. \n",
    "\n",
    "#### Goals (2):\n",
    "\n",
    "- For $p = 2, 10, 100$, generate $1000$ samples of $Y$ and $Z$ and create a histogram of the resulting dot products $\\widehat{Y} \\cdot \\widehat{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction using random projections.\n",
    "\n",
    "As we saw above, high dimensional data can cause quite a few problems. We'll now explore a few ways of reducing dimensionality. \n",
    "\n",
    "Suppose again that we have a $p$-dimensional dataset for large $p$, and we want to reduce the number of dimensions to $n$ for $n << p$. One way of approaching this problem is via random projections. A random projection $M$ in this case would be a $p \\times n$ matrix whose entries are chosen at random. If $u \\in \\mathbb{R}^p$ is a point in our dataset, we compute its projection $\\widehat{u} \\in \\mathbb{R}^n$ with the following matrix multiplication:\n",
    "\n",
    "$$\n",
    "\\widehat{u} = u^T \\cdot M.\n",
    "$$\n",
    "\n",
    "However, we don't want to choose just any random matrix $M$. Ideally, we would like our projection to preserve distances between points in our dataset. We can do this by choosing the matrix $M$ to be one of the following kinds:\n",
    "\n",
    "- A **Gaussian random projection** is given by drawing the entries of $M$ from the normal distrbution $\\mathcal{N}(0, 1/n)$ (recall $n$ is the dimension of the target space, also referred to as the number of components of the projection).\n",
    "\n",
    "- An **Achlioptas random projection** is given by drawing the entries of $M_{ij}$ of $M$ according to \n",
    "\n",
    "$$\n",
    "M_{ij} = \\begin{cases}\n",
    "  \\sqrt{3}  \\quad &\\text{with probability } &\\frac{1}{6} \\\\\n",
    "  0         \\quad &\\text{..}                &\\frac{2}{3} \\\\\n",
    "  -\\sqrt{3} \\quad &\\text{..}                &\\frac{1}{6}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "If $u, v \\in \\mathbb{R}^p$ are points in our dataset, then projections like those above satisfy bounds of the form\n",
    "\n",
    "$$\n",
    "(1 - \\epsilon) \\| u - v \\|^2\n",
    "  \\leq\n",
    "\\| Mu - Mv \\|^2\n",
    "  \\leq\n",
    "(1 + \\epsilon) \\|u - v\\|^2.\n",
    "$$\n",
    "\n",
    "The relationship between the dimension $n$ of the target space, the number of samples in our dataset, and $\\epsilon$ is given by\n",
    "\n",
    "$$\n",
    " n = \\frac{4\\log(\\text{number of samples})}{\\epsilon^2/2 - \\epsilon^3 /3}.\n",
    "$$\n",
    "\n",
    "Typically the number of samples is given, and so we just compute $n$ as a function of $\\epsilon$. \n",
    "\n",
    "#### Goals (3)\n",
    "\n",
    "- Supposing we have a $10000$-dimensional dataset with only $10$ samples, and we want $\\epsilon = 0.1$, verify that the Gaussian and Achlioptas projections satisfy the bound given above (with these numbers, we need to choose $n$ > 857). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
